{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b549b58-11d6-493d-88da-22b5e73e8d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark\n",
    "!pip install findspark\n",
    "!pip install pyspark numpy pandas matplotlib seaborn scikit-lear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31011334-a691-4512-8432-b33719055c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef8382-661e-4634-b101-44b5dd5835e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"HeartDiseasePrediction\").getOrCreate()\n",
    "\n",
    "# Load dataset (ensure the CSV file is accessible in your environment)\n",
    "df = spark.read.csv(\"C:/Users/sutha/Desktop/heart_2020_cleaned.csv\", header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35a3b5d-cb87-4f8e-af66-75714284a07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df_pd = df.select(\"HeartDisease\").toPandas()\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(data=df_pd, x=\"HeartDisease\", palette=\"coolwarm\")\n",
    "plt.xlabel(\"Heart Disease (0 = No, 1 = Yes)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Heart Disease Cases\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744ec3af-51d5-4431-b045-5ff0f009b45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the target variable: map \"Yes\" to 1 and \"No\" to 0\n",
    "df = df.withColumn(\"HeartDisease\", when(col(\"HeartDisease\") == \"Yes\", 1).otherwise(0))\n",
    "\n",
    "# Convert binary columns from Yes/No to 1/0\n",
    "binary_cols = ['Smoking', 'AlcoholDrinking', 'Stroke', 'DiffWalking',\n",
    "               'PhysicalActivity', 'Asthma', 'KidneyDisease', 'SkinCancer']\n",
    "for c in binary_cols:\n",
    "    df = df.withColumn(c, when(col(c) == \"Yes\", 1).otherwise(0))\n",
    "\n",
    "# Categorical columns to be indexed and one-hot encoded\n",
    "categorical_cols = ['Sex', 'AgeCategory', 'Race', 'Diabetic', 'GenHealth']\n",
    "\n",
    "# Create indexers and encoders for each categorical variable\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c+\"_Index\", handleInvalid=\"keep\") for c in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=c+\"_Index\", outputCol=c+\"_OHE\") for c in categorical_cols]\n",
    "\n",
    "# Numerical columns\n",
    "numerical_cols = ['BMI', 'PhysicalHealth', 'MentalHealth', 'SleepTime']\n",
    "\n",
    "# Assemble features: include one-hot encoded categorical columns, binary columns, and numerical columns\n",
    "assemblerInputs = [c+\"_OHE\" for c in categorical_cols] + binary_cols + numerical_cols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96df668b-4f6d-48d4-8699-ec3d04cd729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the preprocessing pipeline\n",
    "pipeline_stages = indexers + encoders + [assembler]\n",
    "pipeline = Pipeline(stages=pipeline_stages)\n",
    "pipelineModel = pipeline.fit(df)\n",
    "df_transformed = pipelineModel.transform(df)\n",
    "\n",
    "# Select only the features and label\n",
    "data = df_transformed.select(\"features\", \"HeartDisease\")\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d11d255-e5ad-402b-ad58-fe323e4fb5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training (80%) and testing (20%) sets\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Initialize BinaryClassificationEvaluator using areaUnderROC as the metric\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"HeartDisease\", metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8103158f-f608-4e93-9bf2-6f2080a45913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "def train_model(model, paramGrid):\n",
    "    # Build a Pipeline with the model as the final stage (features already preprocessed)\n",
    "    pipeline_model = Pipeline(stages=[model])\n",
    "\n",
    "    # Set up CrossValidator with 5-fold CV\n",
    "    cv = CrossValidator(estimator=pipeline_model,\n",
    "                        estimatorParamMaps=paramGrid,\n",
    "                        evaluator=evaluator,\n",
    "                        numFolds=5,\n",
    "                        parallelism=2)\n",
    "\n",
    "    # Fit the model on training data\n",
    "    cvModel = cv.fit(train_data)\n",
    "    # Generate predictions on test data\n",
    "    predictions = cvModel.transform(test_data)\n",
    "    # Evaluate using AUC\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "    return cvModel, predictions, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5272fe-24d2-4467-aa64-c7c94aa0e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"HeartDisease\", maxIter=100)\n",
    "paramGrid_lr = ParamGridBuilder().addGrid(lr.regParam, [0.01, 0.1]).addGrid(lr.elasticNetParam, [0.0, 0.5]).build()\n",
    "\n",
    "cvModel_lr, predictions_lr, auc_lr = train_model(lr, paramGrid_lr)\n",
    "print(\"Logistic Regression AUC:\", auc_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b401957f-6879-45e9-94d4-eeb17d64ea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# Initialize evaluators\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"HeartDisease\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "evaluator_precision = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"HeartDisease\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"\n",
    ")\n",
    "\n",
    "evaluator_recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"HeartDisease\", predictionCol=\"prediction\", metricName=\"weightedRecall\"\n",
    ")\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"HeartDisease\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "\n",
    "evaluator_auc = BinaryClassificationEvaluator(\n",
    "    labelCol=\"HeartDisease\", metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = evaluator_accuracy.evaluate(predictions_lr)\n",
    "precision = evaluator_precision.evaluate(predictions_lr)\n",
    "recall = evaluator_recall.evaluate(predictions_lr)\n",
    "f1_score = evaluator_f1.evaluate(predictions_lr)\n",
    "auc = evaluator_auc.evaluate(predictions_lr)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1_score:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3931c83-1705-4803-b2f9-7cc28c5f0834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"HeartDisease\")\n",
    "paramGrid_dt = ParamGridBuilder().addGrid(dt.maxDepth, [5, 10, 15]).addGrid(dt.minInstancesPerNode, [1, 5]).build()\n",
    "\n",
    "cvModel_dt, predictions_dt, auc_dt = train_model(dt, paramGrid_dt)\n",
    "print(\"Decision Tree AUC:\", auc_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d9242e-6eef-47cb-aca8-16b842c0852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# Initialize evaluators\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"HeartDisease\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "evaluator_precision = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"HeartDisease\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"\n",
    ")\n",
    "\n",
    "evaluator_recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"HeartDisease\", predictionCol=\"prediction\", metricName=\"weightedRecall\"\n",
    ")\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"HeartDisease\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "\n",
    "evaluator_auc = BinaryClassificationEvaluator(\n",
    "    labelCol=\"HeartDisease\", metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = evaluator_accuracy.evaluate(predictions_dt)\n",
    "precision = evaluator_precision.evaluate(predictions_dt)\n",
    "recall = evaluator_recall.evaluate(predictions_dt)\n",
    "f1_score = evaluator_f1.evaluate(predictions_dt)\n",
    "auc = evaluator_auc.evaluate(predictions_dt)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1_score:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa52f9c-2db7-4277-8852-059498b1cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"HeartDisease\")\n",
    "paramGrid_rf = ParamGridBuilder().addGrid(rf.numTrees, [50, 100]).addGrid(rf.maxDepth, [5, 10]).build()\n",
    "\n",
    "cvModel_rf, predictions_rf, auc_rf = train_model(rf, paramGrid_rf)\n",
    "print(\"Random Forest AUC:\", auc_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27195767-ba60-4fbd-98e9-b6128477b561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# Initialize evaluators\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"HeartDisease\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "evaluator_precision = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"HeartDisease\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"\n",
    ")\n",
    "\n",
    "evaluator_recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"HeartDisease\", predictionCol=\"prediction\", metricName=\"weightedRecall\"\n",
    ")\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"HeartDisease\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "\n",
    "evaluator_auc = BinaryClassificationEvaluator(\n",
    "    labelCol=\"HeartDisease\", metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = evaluator_accuracy.evaluate(predictions_rf)\n",
    "precision = evaluator_precision.evaluate(predictions_rf)\n",
    "recall = evaluator_recall.evaluate(predictions_rf)\n",
    "f1_score = evaluator_f1.evaluate(predictions_rf)\n",
    "auc = evaluator_auc.evaluate(predictions_rf)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1_score:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "####\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"HeartDisease\", maxIter=50)\n",
    "paramGrid_gbt = ParamGridBuilder().addGrid(gbt.maxDepth, [5, 10]).build()\n",
    "\n",
    "cvModel_gbt, predictions_gbt, auc_gbt = train_model(gbt, paramGrid_gbt)\n",
    "print(\"Gradient-Boosted Trees AUC:\", auc_gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f605c0bc-b396-4817-b1bf-bc38878fd384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# Initialize evaluators\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"HeartDisease\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "evaluator_precision = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"HeartDisease\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"\n",
    ")\n",
    "\n",
    "evaluator_recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"HeartDisease\", predictionCol=\"prediction\", metricName=\"weightedRecall\"\n",
    ")\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"HeartDisease\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "\n",
    "evaluator_auc = BinaryClassificationEvaluator(\n",
    "    labelCol=\"HeartDisease\", metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = evaluator_accuracy.evaluate(predictions_gbt)\n",
    "precision = evaluator_precision.evaluate(predictions_gbt)\n",
    "recall = evaluator_recall.evaluate(predictions_gbt)\n",
    "f1_score = evaluator_f1.evaluate(predictions_gbt)\n",
    "auc = evaluator_auc.evaluate(predictions_gbt)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1_score:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b14bd72-a9e0-4c76-b7d6-f8bd503d3628",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC Comparison:\")\n",
    "print(\"Logistic Regression:\", auc_lr)\n",
    "print(\"Decision Tree:\", auc_dt)\n",
    "print(\"Random Forest:\", auc_rf)\n",
    "print(\"GBT:\", auc_gbt)\n",
    "\n",
    "# Choose the best model based on highest AUC\n",
    "auc_dict = {\"LogisticRegression\": auc_lr, \"DecisionTree\": auc_dt, \"RandomForest\": auc_rf, \"GBT\": auc_gbt}\n",
    "best_model_name = max(auc_dict, key=auc_dict.get)\n",
    "print(\"Best Model:\", best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690d9b8f-71ca-4e4a-8dbc-df6a366c56d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Function to compute correlation\n",
    "def compute_corr(predictions, model_name):\n",
    "    # Convert to Pandas DataFrame\n",
    "    predictions_pd = predictions.select(\"HeartDisease\", \"probability\").toPandas()\n",
    "    \n",
    "    # Extract probability of having heart disease (second value in probability vector)\n",
    "    predictions_pd[\"Predicted_Prob\"] = predictions_pd[\"probability\"].apply(lambda x: float(x[1]))\n",
    "\n",
    "    # Compute Pearson and Spearman Correlation\n",
    "    pearson_corr = stats.pearsonr(predictions_pd[\"HeartDisease\"], predictions_pd[\"Predicted_Prob\"])[0]\n",
    "    spearman_corr = stats.spearmanr(predictions_pd[\"HeartDisease\"], predictions_pd[\"Predicted_Prob\"])[0]\n",
    "\n",
    "    print(f\"**{model_name} Correlations:**\")\n",
    "    print(f\" - Pearson Correlation: {pearson_corr:.4f}\")\n",
    "    print(f\" - Spearman Correlation: {spearman_corr:.4f}\")\n",
    "    print()\n",
    "\n",
    "    return pearson_corr, spearman_corr\n",
    "\n",
    "# Compute correlation for all models\n",
    "corr_lr = compute_corr(predictions_lr, \"Logistic Regression\")\n",
    "corr_dt = compute_corr(predictions_dt, \"Decision Tree\")\n",
    "corr_rf = compute_corr(predictions_rf, \"Random Forest\")\n",
    "corr_gbt = compute_corr(predictions_gbt, \"Gradient-Boosted Trees\")\n",
    "\n",
    "# Store results in a dictionary for easy comparison\n",
    "corr_results = {\n",
    "    \"Logistic Regression\": corr_lr,\n",
    "    \"Decision Tree\": corr_dt,\n",
    "    \"Random Forest\": corr_rf,\n",
    "    \"Gradient-Boosted Trees\": corr_gbt\n",
    "}\n",
    "\n",
    "# Print best model based on highest correlation\n",
    "best_corr_model = max(corr_results, key=lambda x: corr_results[x][0])  # Pearson is the primary metric\n",
    "print(f\"Best Model Based on Pearson Correlation: {best_corr_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edb1c38-d61e-4f1f-86c6-adc124ed5ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "\n",
    "# Choose best predictions based on the best model selected\n",
    "if best_model_name == \"LogisticRegression\":\n",
    "    best_predictions = predictions_lr\n",
    "elif best_model_name == \"DecisionTree\":\n",
    "    best_predictions = predictions_dt\n",
    "elif best_model_name == \"RandomForest\":\n",
    "    best_predictions = predictions_rf\n",
    "elif best_model_name == \"GBT\":\n",
    "    best_predictions = predictions_gbt\n",
    "\n",
    "# Define a UDF to convert predicted probability vector to a risk score (0-100)\n",
    "def prob_to_score(probability):\n",
    "    return float(probability[1] * 100)\n",
    "prob_to_score_udf = udf(prob_to_score, DoubleType())\n",
    "\n",
    "best_predictions = best_predictions.withColumn(\"RiskScore\", prob_to_score_udf(col(\"probability\")))\n",
    "\n",
    "# Define risk categories based on score thresholds\n",
    "def risk_category(score):\n",
    "    if score < 33:\n",
    "        return \"Low\"\n",
    "    elif score < 66:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"High\"\n",
    "risk_category_udf = udf(risk_category, StringType())\n",
    "best_predictions = best_predictions.withColumn(\"RiskCategory\", risk_category_udf(col(\"RiskScore\")))\n",
    "\n",
    "best_predictions.select(\"HeartDisease\", \"RiskScore\", \"RiskCategory\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8518849d-05f1-48b4-96e3-202b58c5d8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "best_predictions_pd = best_predictions.select(\"RiskScore\").toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(best_predictions_pd[\"RiskScore\"], bins=30, color=\"skyblue\", edgecolor=\"black\", alpha=0.7)\n",
    "plt.xlabel(\"Risk Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Predicted Risk Scores\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f02b85-129c-4a9d-a475-0be66383e5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_category_pd = best_predictions.select(\"RiskCategory\").toPandas()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(data=risk_category_pd, x=\"RiskCategory\", palette=\"coolwarm\", order=[\"Low\", \"Medium\", \"High\"])\n",
    "plt.xlabel(\"Risk Category\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Risk Categories\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777442f0-afcc-43ff-b484-0b00eefbc177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt \n",
    "def get_roc_data(predictions):\n",
    "    pred_pd = predictions.select(\"probability\", \"HeartDisease\").toPandas()\n",
    "    y_true = pred_pd[\"HeartDisease\"]\n",
    "    y_scores = pred_pd[\"probability\"].apply(lambda x: x[1])\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    return fpr, tpr, roc_auc\n",
    "\n",
    "fpr_lr, tpr_lr, auc_lr = get_roc_data(predictions_lr)\n",
    "fpr_dt, tpr_dt, auc_dt = get_roc_data(predictions_dt)\n",
    "fpr_rf, tpr_rf, auc_rf = get_roc_data(predictions_rf)\n",
    "fpr_gbt, tpr_gbt, auc_gbt = get_roc_data(predictions_gbt)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_lr, tpr_lr, label=f\"Logistic Regression (AUC = {auc_lr:.2f})\")\n",
    "plt.plot(fpr_dt, tpr_dt, label=f\"Decision Tree (AUC = {auc_dt:.2f})\")\n",
    "plt.plot(fpr_rf, tpr_rf, label=f\"Random Forest (AUC = {auc_rf:.2f})\")\n",
    "plt.plot(fpr_gbt, tpr_gbt, label=f\"GBT (AUC = {auc_gbt:.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Random Guessing\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve Comparison\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
